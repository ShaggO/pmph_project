\documentclass[]{report}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=c++,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
% Title Page
\title{}
\author{}


\begin{document}
\section{CPU Parallelization Preparation}
The first step for us was to begin preparing the CPU for kernel parallelization. Before distributing the outer loops in ProjCoreOrig.cpp,
 we began by moving all of the secondary functions (\texttt{void updateParams}, \texttt{void setPayoff}, \texttt{REAL value}, and \texttt{void rollback})into \texttt{int main}. This allowed us to easily see the globs (global variables) and their relations to one another.
 
 Conglomerating the separate functions into one allowed us to hoist the initialization of the globs into glob arrays. That is, instead of initializing a temp variable or array for every iteration of a loop, we instead initialize an array one dimension larger, with size equal to the number of iterations of the loop, before the looping code. See figures 1 and 2 for an example. 
 \begin{figure}
 \begin{lstlisting}
  for (int i = 0; i<max; i++){
  	float tmpA = 0.0;
  	for (int j = 0; j<max2; j++){
  		tmpA += 2*B[j];
  		...
  		}
  	...
  }
 \end{lstlisting}
 \caption{ A code snippet with tmpA initialized for every iteration.}
 \end{figure}
 \begin{figure}
 \begin{lstlisting}
  float tmpA[max] = {0.0, ...};
  for (int i = 0; i<max; i++){
  	for (int j = 0; j<max2; j++){
  		tmpA[i] += 2*B[j];
  		...
  		}
  	...
  }
 \end{lstlisting}
 \caption{ The same code with tmpA hoisted.}
 \end{figure}
  
The purpose of this is to allow us to easily parallelize the inner loops. Since each iteration of the inner \texttt{j} loop requires access to a single \texttt{tmpA} per iteration of the \texttt{i} loop, we cannot distribute the inner loops in the original implementation. In the hoisted version, however, 
\subsection{section notes while writing}
the initialization of the glob array has been hoisted out
prev, every iteration had an init of globs, now we have an array of globs and it is initialized beforehand, in each init it was init to same val
first create init, create array where defaullt is the init value.
\section{Loop}

\section{Simple CUDA}

\section{TRIDAG}


\end{document}          
